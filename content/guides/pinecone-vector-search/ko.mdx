# Pinecone에서 벡터 검색 구현 - 의미 검색 구축

이제 Pinecone 인덱스가 있으니, 벡터로 채우고 의미 검색을 수행해봅시다. 25분 안에 의미 검색 시스템을 구축하세요!

<FreeTierInfo
  service="Pinecone 벡터 검색"
  limit="100K 벡터까지 무제한 쿼리"
  period="영구 무료"
  status="generous"
/>

## 배우게 될 내용

<Callout type="info">
🔍 **이 가이드의 끝에서:**

- 문서에서 텍스트 임베딩 만들기
- 벡터를 Pinecone에 업로드
- 유사한 항목에 대해 인덱스 쿼리
- 의미 검색 애플리케이션 구축
- 문서 유사성 매칭 구현
- 대규모 벡터 작업 처리
</Callout>

## 시작하기 전에

<PrerequisiteCheck items={[
  { text: "Pinecone 계정 및 인덱스 (이전 가이드)", required: true },
  { text: "Python 3.8+ 및 pinecone-client 설치됨", required: true },
  { text: "OpenAI API 키 (또는 Hugging Face)", required: true },
  { text: "샘플 문서 또는 텍스트 데이터", required: false }
]} />

---

<Step number={1} title="텍스트에서 임베딩 생성">

### Step 1.1: 문서에서 임베딩 생성

`create_embeddings.py` 만들기:

<CopyBlock
  code="from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# 샘플 문서
documents = [
  'Machine learning is a subset of artificial intelligence',
  'Neural networks are inspired by biological neurons',
  'Deep learning uses multiple layers of abstraction',
  'Python is a popular programming language',
  'Data science combines statistics and programming'
]

def create_embedding(text):
  response = client.embeddings.create(
    input=text,
    model='text-embedding-3-small'
  )
  return response.data[0].embedding

# 모든 문서의 임베딩 생성
embeddings = []
for doc in documents:
  embedding = create_embedding(doc)
  embeddings.append({
    'text': doc,
    'embedding': embedding,
    'id': str(hash(doc))
  })

print(f'Created {len(embeddings)} embeddings')
for emb in embeddings:
  print('  -', emb['text'][:50] + '...')"
  language="python"
/>

<Screenshot
  src="https://res.cloudinary.com/demo/image/upload/pinecone-vector-search-step1-ko.png"
  alt="임베딩 생성"
  caption="텍스트를 저장할 준비가 된 벡터 임베딩으로 변환"
/>

### Step 1.2: 효율성을 위한 배치 임베딩

여러 문서의 경우:

<CopyBlock
  code="from openai import OpenAI

client = OpenAI()

# 한 번에 여러 텍스트 배치 (더 효율적)
documents = ['text1', 'text2', 'text3']

response = client.embeddings.create(
  input=documents,
  model='text-embedding-3-small'
)

embeddings = [item.embedding for item in response.data]
print(f'Batch created {len(embeddings)} embeddings')"
  language="python"
/>

<Callout type="tip">
💡 **임베딩 모델:**

- text-embedding-3-small: 1536 차원, 빠름
- text-embedding-3-large: 3072 차원, 더 정확
- Hugging Face all-MiniLM: 384 차원, 경량
</Callout>

<Checkpoint
  title="임베딩 생성"
  items={[
    "임베딩을 만들 수 있나요?",
    "임베딩이 올바른 차원을 가지고 있나요?",
    "문서를 배치로 처리할 수 있나요?"
  ]}
/>

</Step>

---

<Step number={2} title="Pinecone에 벡터 업로드">

### Step 2.1: 연결 및 업로드

`upload_to_pinecone.py` 만들기:

<CopyBlock
  code="import os
from dotenv import load_dotenv
from pinecone import Pinecone
from openai import OpenAI

load_dotenv()

# Pinecone 초기화
pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
index = pc.Index(os.getenv('PINECONE_INDEX_NAME'))

# OpenAI 초기화
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# 샘플 문서
documents = [
  {'id': '1', 'text': 'Machine learning is a subset of AI'},
  {'id': '2', 'text': 'Neural networks are inspired by brains'},
  {'id': '3', 'text': 'Python is popular for data science'}
]

# 임베딩 생성
vectors_to_upsert = []
for doc in documents:
  response = client.embeddings.create(
    input=doc['text'],
    model='text-embedding-3-small'
  )
  embedding = response.data[0].embedding

  vectors_to_upsert.append({
    'id': doc['id'],
    'values': embedding,
    'metadata': {'text': doc['text']}
  })

# Pinecone에 업로드
index.upsert(vectors=vectors_to_upsert)
print(f'Uploaded {len(vectors_to_upsert)} vectors')"
  language="python"
/>

<Screenshot
  src="https://res.cloudinary.com/demo/image/upload/pinecone-vector-search-step2-ko.png"
  alt="벡터 업로드"
  caption="벡터가 Pinecone 인덱스에 성공적으로 저장됨"
/>

### Step 2.2: Upsert 작업 이해

<CopyBlock
  code="Upsert (업데이트/삽입) 동작:

- ID가 존재하면: 벡터 업데이트
- ID가 없으면: 새 벡터 삽입
- 원자적 작업: 중복 없음
- 대량 작업에 효율적

형식:
[
  {
    'id': 'unique-id',
    'values': [0.1, 0.2, 0.3, ...],  # 벡터
    'metadata': {'key': 'value'}      # 추가 정보
  }
]"
  language="text"
/>

<Callout type="warning">
⚠️ **업로드 제한**

- 요청당 최대 100개 벡터
- 더 큰 배치의 경우 페이지네이션
- 속도 제한을 피하기 위해 배치 사이에 대기
</Callout>

</Step>

---

<Step number={3} title="쿼리 및 유사 벡터 찾기">

### Step 3.1: 기본 벡터 검색

`search.py` 만들기:

<CopyBlock
  code="import os
from dotenv import load_dotenv
from pinecone import Pinecone
from openai import OpenAI

load_dotenv()

# 초기화
pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
index = pc.Index(os.getenv('PINECONE_INDEX_NAME'))
client = OpenAI()

# 쿼리 텍스트
query = 'What is deep learning?'

# 쿼리 임베딩 생성
query_response = client.embeddings.create(
  input=query,
  model='text-embedding-3-small'
)
query_embedding = query_response.data[0].embedding

# 유사 벡터 검색
results = index.query(
  vector=query_embedding,
  top_k=5,
  include_metadata=True
)

# 결과 표시
print(f'Query: {query}')
print('\nMost similar documents:')
for match in results['matches']:
  print(f'  Score: {match['score']:.2f}')
  print(f'  Text: {match['metadata']['text']}')"
  language="python"
/>

<Screenshot
  src="https://res.cloudinary.com/demo/image/upload/pinecone-vector-search-step3-ko.png"
  alt="검색 결과"
  caption="쿼리는 관련성 점수가 있는 가장 유사한 벡터를 반환합니다"
/>

### Step 3.2: 검색 결과 이해

<CopyBlock
  code="검색 응답 구조:

{
  'matches': [
    {
      'id': 'vector-id',
      'score': 0.92,              # 코사인 유사성 0-1
      'values': [...],            # 벡터 값 (선택사항)
      'metadata': {               # 저장된 메타데이터
        'text': 'Document text'
      }
    }
  ],
  'namespace': 'default'
}

점수 해석:
- 1.0: 완벽한 일치
- 0.8-1.0: 매우 유사
- 0.5-0.8: 다소 관련
- 0.5 미만: 약하게 관련"
  language="text"
/>

<Callout type="tip">
💡 **쿼리 팁:**

- 원본 텍스트를 보려면 항상 include_metadata 추가
- 사용 사례에 따라 top_k 조정 (일반적으로 3-10)
- 특정 부분 집합에 대해 메타데이터로 필터링
- 낮은 관련성 결과를 제외하려면 점수 임계값 추가
</Callout>

</Step>

---

<Step number={4} title="완전한 의미 검색 앱 구축">

### Step 4.1: 완전한 검색 애플리케이션

`semantic_search_app.py` 만들기:

<CopyBlock
  code="import os
from dotenv import load_dotenv
from pinecone import Pinecone
from openai import OpenAI

class SemanticSearch:
  def __init__(self):
    load_dotenv()
    self.pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
    self.index = self.pc.Index(os.getenv('PINECONE_INDEX_NAME'))
    self.client = OpenAI()

  def add_document(self, doc_id, text):
    '''단일 문서를 인덱스에 추가'''
    response = self.client.embeddings.create(
      input=text,
      model='text-embedding-3-small'
    )
    embedding = response.data[0].embedding

    self.index.upsert([{
      'id': doc_id,
      'values': embedding,
      'metadata': {'text': text}
    }])

  def search(self, query, top_k=5):
    '''유사 문서 검색'''
    response = self.client.embeddings.create(
      input=query,
      model='text-embedding-3-small'
    )
    query_embedding = response.data[0].embedding

    results = self.index.query(
      vector=query_embedding,
      top_k=top_k,
      include_metadata=True
    )

    return results['matches']

# 사용
search_app = SemanticSearch()

# 문서 추가
search_app.add_document('1', 'AI and machine learning basics')
search_app.add_document('2', 'Deep learning with neural networks')
search_app.add_document('3', 'Python programming guide')

# 검색
results = search_app.search('What is machine learning?')
print('Search results:')
for r in results:
  print(f'  {r['metadata']['text']} (Score: {r['score']:.2f})')"
  language="python"
/>

<Screenshot
  src="https://res.cloudinary.com/demo/image/upload/pinecone-vector-search-step4-ko.png"
  alt="완전한 앱 출력"
  caption="의미 검색 애플리케이션이 관련 문서를 반환합니다"
/>

### Step 4.2: 프로덕션 고려사항

<CopyBlock
  code="프로덕션 준비 기능:

1. 오류 처리
   - 네트워크 타임아웃 처리
   - 백오프를 사용한 재시도 로직
   - 우아한 성능 저하

2. 캐싱
   - API 호출을 줄이기 위해 임베딩 캐시
   - 자주 검색된 쿼리 저장

3. 모니터링
   - 검색 지연 시간 추적
   - 쿼리 비용 모니터링
   - 실패한 검색 로깅

4. 확장
   - 대량 문서 배치 처리
   - 데이터 격리를 위해 네임스페이스 사용
   - 큰 결과 집합에 대한 페이지네이션 구현"
  language="text"
/>

<Checkpoint
  title="검색 완료"
  items={[
    "임베딩을 만들 수 있나요?",
    "Pinecone에 업로드할 수 있나요?",
    "쿼리하고 결과를 받을 수 있나요?",
    "의미 검색이 올바르게 작동하나요?"
  ]}
/>

<Callout type="success">
✅ **의미 검색이 작동합니다!** 이제 스마트 검색 애플리케이션을 구축할 수 있습니다.
</Callout>

</Step>

---

## 일반적인 사용 사례

<CopyBlock
  code="인기 있는 의미 검색 애플리케이션:

1. 상품 검색
   - 유사 상품 찾기
   - 추천 엔진
   - 관련 항목 제안

2. 문서 검색
   - 콘텐츠 발견
   - 유사성 매칭
   - 연구 어시스턴트

3. 질문 답변
   - FAQ 매칭
   - 관련 문서 찾기
   - 지원 티켓 라우팅

4. 코드 검색
   - 유사 함수 찾기
   - 중복 감지
   - 코드 재사용 발견

5. 추천 시스템
   - 사용자 선호도 매칭
   - 콘텐츠 개인화
   - 교차 판매 제안"
  language="text"
/>

---

## 최적화 팁

<Callout type="tip">
⚡ **검색 품질 개선:**

1. **더 나은 임베딩**
   - 더 나은 정확도를 위해 더 강력한 모델 사용
   - 다양한 임베딩 차원 시도

2. **데이터 전처리**
   - 텍스트 정리 및 정규화
   - 중복 제거
   - 관련 메타데이터 추가

3. **검색 조정**
   - 재현율을 위해 top_k 조정
   - 메타데이터 필터 추가
   - 재순위 지정 구현

4. **성능**
   - API 호출 배치 처리
   - 결과 캐시
   - 비용 모니터링
</Callout>

---

## 다음 단계

1. **[LangChain과 RAG](../langchain-rag-setup)** - LLM과 결합
2. **[하이브리드 검색](../pinecone-hybrid-search)** - 벡터와 키워드 검색 혼합
3. **[고급 인덱싱](../pinecone-namespaces)** - 네임스페이스로 정렬

---

**의미 검색이 활성화되었습니다! 이제 지능형 애플리케이션을 강화할 시간입니다!** 🚀
